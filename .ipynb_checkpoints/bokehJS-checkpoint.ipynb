{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title : Categorization and search engine for the Covid-19 research papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About project: \n",
    "\n",
    "During this Covid-19 pandemic, there is a urgent need of proper medication to eradicate the disease from our planet. Health care professionals and researchers around the world working continuously in order to discover the vaccine. In this process, the researchers have been publishing their ideas and ways to deal with this kind of virus. It is difficult to keep up with these numerous articles being published daily. This project mainly focus on solving the problem of identifying the groups (categorize) of articles and the important topics or keywords for each those groups, which helps us to categorize the articles and focus more on the those that are of high interest. Further, this project also aims at building a web application to provide better interactivity with the models built in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why I did this project:\n",
    "\n",
    "Companies, Eductional institutions and many organizations are being flooded with large corpora of text daily and they need data science tools to draw valuable insights from the data and take key decisions to develop business and inturn improve the customer satisfaction towards the company's product. This project aims at dealing with similar kind of problem except that we are doing this project with covid-19 research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow of project:\n",
    "\n",
    "Below is the diagram depicting the different stages in data science life cycle. I will explain different techniques corresponding to each stage after the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "image/png": {
       "height": 500,
       "width": 1000
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename = 'imgs/data-science.png',height = 500, width = 1000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description: \n",
    "\n",
    "Dataset is taken from kaggle: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
    "\n",
    "In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 187,000 scholarly articles, including over 82,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the analysis\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import json\n",
    "\n",
    "## For basic visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "\n",
    "- It is quite intimidating to see all the data collected. In my perspective, it is always better to understand the data that we have in hand first before jumping into any further steps. \n",
    "\n",
    "- So, load the metadata file and know about the available data at high-level and reason whether we can solve problem with this available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "metadata_path = f'{root_path}/input_data/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis - Transform and visualization\n",
    "\n",
    "Hint: Iteratively, ask questions about the data and find the solutions to those questions\n",
    "\n",
    "- Mostly, we will be using the \"abstract\" of each research paper, due to the fact that the authors of article will briefly summarize about their work in the form of abstract and it is more efficient in terms of computation requirement.\n",
    "\n",
    "- Analyse the abstracts written by the authors and remove if there any duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the unique records match the number of actual records.\n",
    "\n",
    "meta_df['abstract'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove duplicate records\n",
    "\n",
    "meta_df.drop_duplicates(['abstract'] ,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Subset the samples randomly (since, running on the whole dataset is not feasible in kaggle)\n",
    "\n",
    "subset_data_df = meta_df.sample(10000, random_state = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove the records with NaN values\n",
    "\n",
    "subset_data_df.dropna(how='all', subset = ['abstract', 'title', 'authors'], inplace = True)\n",
    "subset_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, what about the different languages in our text?\n",
    "\n",
    "- Get the distribution plot explaining the number of records that belong to each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install spacy-langdetect\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def language_detection(df, column_name = None , num_tokens = 50,  plot = True):\n",
    "    \"\"\"\n",
    "    Takes in the dataframe and output the dataframe with languages for each record based on the mentioned text column name with starting \"num_tokens\" words\n",
    "    \"\"\"\n",
    "\n",
    "    # hold label - language\n",
    "    languages = []\n",
    "\n",
    "    # go through each text\n",
    "    for idx in tqdm(range(0,len(df))):\n",
    "        # split by space into list, take the first x intex, join with space\n",
    "        text = df.iloc[idx][column_name]\n",
    "        \n",
    "        nlp = spacy.load('en')\n",
    "        \n",
    "        ## since, it is time consuming, we detect the language using only starting \"num_tokens\"\n",
    "        initial_text = text.split(' ')[:num_tokens]\n",
    "        \n",
    "        nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "        # process the text\n",
    "        doc = nlp(\" \".join(initial_text))\n",
    "        \n",
    "        # get the language  \n",
    "        lang = doc._.language['language']\n",
    "        languages.append(lang)\n",
    "    \n",
    "    df['language'] = languages\n",
    "    languages_dict = {}\n",
    "    if plot == True :\n",
    "        for lang in set(languages):\n",
    "            languages_dict[lang] = languages.count(lang)\n",
    "\n",
    "        print(\"Total: {}\\n\".format(len(languages)))\n",
    "        pprint(languages_dict)\n",
    "        plt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\n",
    "        plt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\n",
    "        plt.title(\"Distribution of Languages in Dataset\")\n",
    "        plt.xlabel(\"Languages\")\n",
    "        plt.ylabel(\"Number of records\")\n",
    "        plt.show()\n",
    "    return df, languages, languages_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lang_detected_data_df, lang_series, lang_dict = language_detection(subset_data_df, column_name = 'abstract', num_tokens = 50, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the dataset for only english records\n",
    "\n",
    "filtered_data_df = lang_detected_data_df[lang_detected_data_df['language'] == 'en']\n",
    "filtered_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df.to_csv('/output/filtered_data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df = pd.read_csv('./output/filtered_data_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common issue dealing with text : What about the stop words ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "stop_words = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI', 'www', 'objective', 'abstract'\n",
    "]\n",
    "\n",
    "for w in custom_stop_words:\n",
    "    if w not in stop_words:\n",
    "        stop_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spacy bio parser \n",
    "# Ref: https://allenai.github.io/scispacy/\n",
    "\n",
    "# from IPython.utils import io\n",
    "# with io.capture_output() as captured:\n",
    "#     !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scispacy\n",
    "# !pip install en_core_sci_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import en_core_sci_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "parser.max_length = 7000000\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply the tokenizer function to the text data\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "filtered_data_df[\"processed_text\"] = filtered_data_df[\"abstract\"].progress_apply(spacy_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, what's the \"number of total words per document\" over all articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count the number of words that really useful for further stages\n",
    "\n",
    "filtered_data_df['words_count'] = filtered_data_df['processed_text'].apply(lambda x : len(x.strip().split()))\n",
    "\n",
    "\n",
    "filtered_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLot the counts to know the distribution of words per document\n",
    "sns.distplot(filtered_data_df['words_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print from summary statistics\n",
    "\n",
    "filtered_data_df['words_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At average, there are 106 (median) words per article. The average number of words for all articles is 108 (mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can ask many more questions like this... and get the answers through analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize(text, max_num_features ):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_num_features)\n",
    "    text_vectorized = vectorizer.fit_transform(text)\n",
    "    return text_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = filtered_data_df['processed_text'].values\n",
    "\n",
    "#### \n",
    "median_from_distplot = 106\n",
    "# -------------------------\n",
    "\n",
    "text_vectorized_csr = vectorize(text, median_from_distplot)\n",
    "text_vectorized_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert CSR matrix to NdArray\n",
    "text_vectorized_df = text_vectorized_csr.toarray()\n",
    "\n",
    "print(text_vectorized_df.shape)\n",
    "print(type(text_vectorized_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### All models are wrong, but some are useful - George Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Basically, clustering is done so that we partition the data samples into groups (a.k.a clusters) such that we follow two criteria :\n",
    "\n",
    "1. Cohesion - the samples in a cluster are closer\n",
    "\n",
    "2. Seperation - clusters are far away from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will be going through the clustering process with the KMeans. Next, we will reduce the dimension of the feature space, for now we are passing the entire data into each algorithm and looking for the right number of clusters.\n",
    "\n",
    "Clustering methods that are applied here includes: \n",
    "\n",
    "1. KMeans\n",
    "    \n",
    "    i. Random centroid initialization\n",
    "    \n",
    "    ii. KMeans++ : centroids are initialized far away from each other \n",
    "        \n",
    "2. Hierarchical (agglomerative or divisive)\n",
    "\n",
    "3. GMM\n",
    "\n",
    "4. Spectral Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A Kmeans and some metrics :\n",
    "\n",
    "**How many clusters?**\n",
    "\n",
    "To find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters.\n",
    "\n",
    "**Inertia Plot-** (Also, called distortion or Within-cluster Sum of Squares(WSS) )\n",
    "\n",
    "- Below is a plot of the inertia decrese as we increase the number of clusters. Remember we want our inertia to be LOW. A low inertia means our clusters are very tightly packed (cohesive). Generally, we pick number of clusters where the change in distortion is relatively low (Elbow method). There is no hard elbow, so we pick a good number of clusters to be 6.(This is based on the diminishing decrease of inertia.)\n",
    "\n",
    "**Silhouette Scores -** \n",
    "\n",
    "- Next we have silhouette scores for different numbers of clusters. A score of -1 means poor clustering, 0 means cluster overrlap, and 1 means good clusterning. (good clustering is tightly packed clusters that are far away from each other.)\n",
    "\n",
    "**Silhouette Plots-**\n",
    "\n",
    "- Latly we have silhouette plots which are the silhouette scores of every sample within each cluster. Note that imbalanced clusters lead to wider bars. Samples with higher spectrall coefficient are close to their cluster mates, and far from their neighbors in other clusters\n",
    "\n",
    "- the plots with wide fluctuations from cluster to cluster signify that clusters choice is bad. Otherwise, if there is less fluctuation between clusters, then it is good cluster choice.\n",
    "\n",
    "- Coming to thickness, we want silhoutte plots to have similar sized clusters rather than the bulky or thin clusters.\n",
    "\n",
    "**Description of plot:**\n",
    "\n",
    "- Red dashed line is the overall silhoutte score for all the clusters and blocks with different colors represent different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install scikit-learn, if not installed.\n",
    "# pip3 install scikit-learn \n",
    "\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functional implementations for plotting Inertia and silhoutte score for visual interpretation\n",
    "\n",
    "## For Inertia\n",
    "def inertia_plot( model, X, start = 2, stop = 10, kmeans_init = 'random'):\n",
    "    #A simple inertia plotter to decide K in KMeans\n",
    "    inertia = []\n",
    "    for x in range(start,stop):\n",
    "        km = model(n_clusters = x, init = kmeans_init )\n",
    "        labels = km.fit_predict(X)\n",
    "        inertia.append(km.inertia_)\n",
    "    plt.figure(figsize = (10,6))\n",
    "    plt.plot(range(start,stop), inertia, marker = 'o')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Inertia Decrease with K')\n",
    "    plt.xticks(list(range(start, stop)))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "## For Silhoutte score\n",
    "\n",
    "def silhoutte_scores_plots(model,  X, start=2, stop=5, metric = 'euclidean'):\n",
    "    # Taken from sebastian Raschkas book Python Machine Learning second edition\n",
    "    for x in range(start, stop):\n",
    "        km = model(n_clusters = x)\n",
    "        y_km = km.fit_predict(X)\n",
    "        print('Silhouette-Score for', x,  'Clusters: ', silhouette_score(X, y_km))\n",
    "        cluster_labels = np.unique(y_km)\n",
    "        n_clusters = cluster_labels.shape[0]\n",
    "        silhouette_vals = silhouette_samples(X, y_km, metric = metric)\n",
    "        y_ax_lower, y_ax_upper =0,0\n",
    "        yticks = []\n",
    "        for i, c in enumerate(cluster_labels):\n",
    "            c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "            c_silhouette_vals.sort()\n",
    "            y_ax_upper += len(c_silhouette_vals)\n",
    "            color = cm.jet(float(i)/n_clusters)\n",
    "            plt.barh(range(y_ax_lower, y_ax_upper),\n",
    "                    c_silhouette_vals,\n",
    "                    height=1.0,\n",
    "                    edgecolor='none',\n",
    "                    color = color)\n",
    "            yticks.append((y_ax_lower + y_ax_upper)/2.)\n",
    "            y_ax_lower+= len(c_silhouette_vals)\n",
    "\n",
    "        silhouette_avg = np.mean(silhouette_vals)\n",
    "        plt.axvline(silhouette_avg,\n",
    "                   color = 'red',\n",
    "                   linestyle = \"--\")\n",
    "        plt.yticks(yticks, cluster_labels+1)\n",
    "        plt.ylabel(\"cluster\")\n",
    "        plt.xlabel('Silhouette Coefficient')\n",
    "        plt.title('Silhouette for ' + str(x) + \" Clusters\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit KMeans\n",
    "kmeans_model = KMeans\n",
    "print(\"For random seed choice of centroids initialization\")\n",
    "inertia_plot(kmeans_model, text_vectorized_df)\n",
    "\n",
    "\n",
    "print(\"For \\\"kmeans++\\\" seed choice of centroids initialization\")\n",
    "inertia_plot(kmeans_model, text_vectorized_df, kmeans_init = 'k-means++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhoutte_scores_plots(kmeans_model, text_vectorized_df, start = 2, stop = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction for visualization with t-SNE\n",
    "\n",
    "- Using t-SNE we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions such as x,y coordinates, the body_text can be plotted.\n",
    "\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space\n",
    "\n",
    "Other methods :\n",
    "\n",
    "1. UMAP\n",
    "\n",
    "Perplexity : \n",
    "\n",
    "    - In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample\n",
    "    \n",
    "    - Consider selecting a value between 5 and 50. For larger datasets, choose large value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to reduce the dimensions of data\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "def tsne_plot(X, labels = None, k_selected = None, cluster_colors = False):\n",
    "    \n",
    "    # Initialize tsne\n",
    "    tsne = TSNE(verbose=1, perplexity=20, random_state=42)\n",
    "    \n",
    "    X_tsne_embedded = tsne.fit_transform(X)\n",
    "    \n",
    "    # sns settings\n",
    "    sns.set(rc={'figure.figsize':(15,15)})\n",
    "    \n",
    "    if (cluster_colors == False) :\n",
    "        # colors\n",
    "        palette = sns.color_palette(\"bright\", 1)\n",
    "        # plot\n",
    "        sns.scatterplot(X_tsne_embedded[:,0], X_tsne_embedded[:,1], palette=palette)\n",
    "        plt.title('t-SNE with no Labels')\n",
    "        \n",
    "    elif (cluster_colors == True) :\n",
    "        # colors\n",
    "        palette = sns.hls_palette(k_selected, l=.4, s=.9)\n",
    "        # plot\n",
    "        try:\n",
    "            sns.scatterplot(X_tsne_embedded[:,0], X_tsne_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "        except Exception as e:\n",
    "            sns.scatterplot(X_tsne_embedded[:,0], X_tsne_embedded[:,1], hue=labels, legend='full')\n",
    "        plt.title('t-SNE with Cluster Labels')\n",
    "    \n",
    "    plt.show()\n",
    "    return X_tsne_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to plot the samples as scatter plot showing the clusters with different colors\n",
    "def fit_and_plot(model, X, k_selected = None):\n",
    "    \"\"\"This function takes the model and data samples as the input and shows a scatter plot depicting the clusters in samples\"\"\"\n",
    "    # fit the model\n",
    "    # model.fit(X)\n",
    "    # assign a cluster to each example\n",
    "    labels = model.fit_predict(X)\n",
    "    \n",
    "    # Original data samples on tsne-plot\n",
    "    X_tsne_embedded = tsne_plot(X)\n",
    "    \n",
    "    # Clustered data samples on tsne-plot\n",
    "    X_tsne_embedded = tsne_plot(X, labels = labels, k_selected = k_selected, cluster_colors = True)\n",
    "    return X_tsne_embedded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Since, we choose k = 6\n",
    "\n",
    "optimal_k = 6\n",
    "\n",
    "_ = fit_and_plot(kmeans_model(n_clusters = optimal_k), X = text_vectorized_df, k_selected = optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the kmeans is not a right choice and due to large variability in the features, it performs badly on our data. So, we have to reduce the dimensions of data to improve the quality of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction using PCA\n",
    "\n",
    "- In general, Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n",
    "- From here on, we will be using the reduced data in order to maintain the stablility against the \"curse of dimensionality\"\n",
    "\n",
    "Some other methods for Dimensionality reduction are :\n",
    "\n",
    "1. Singular Value Decomposition (SVD) - Also used for the PCA as a solver because it is easy to compute SVD from Gradient descent than computing the PCA from covariance matrix\n",
    "\n",
    "2. Truncated SVD\n",
    "\n",
    "3. CUR\n",
    "\n",
    "3. Normalized Matrix Factorization (NMF)\n",
    "\n",
    "4. MDS\n",
    "\n",
    "5. Kernel PCA\n",
    "\n",
    "6. ISOMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B KMeans with PCA\n",
    "\n",
    "**Note: (Very Important)**\n",
    "\n",
    "- It is really important to standardize the data before doing PCA. Otherwise, the feature with high values range will effect the PCA and they appear to be the principal components (Eigen Vectors) with higher variance (Eigen value).\n",
    "\n",
    "- This standardization really depends on the data that we have in hand and how we would like to feature to represented. Some cases, it is okay to not perform standardization, if we want principal components to take the varied features into account more than others.\n",
    "\n",
    "- Here, we are standardizing to (0,1) range since we have caregorical data from the questions content and we want all of them to be of same scale despite of varied answers from the survey.\n",
    "\n",
    "some feature normalization techniques: \n",
    "\n",
    "1. Standard Normalization (like standard normal distribution) - mean = 0 , variance = 1 $$X_{N} = (X - \\mu)/(\\sigma)$$\n",
    "\n",
    "2. MinMax Scaling - in range of [0, 1]        $$  X_{N} = (X - \\min)/(\\max - \\min) $$\n",
    "\n",
    "3. Mean Normalization - mean = 0 $$X_{N} = (X - \\mu)/(\\max - \\min)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Initialize the min-max scaler and scale the data to (0, 1) range\n",
    "# min_max_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "standard_scaler = StandardScaler(with_mean = True, with_std = True)\n",
    "\n",
    "text_vectorized_df_scaled = standard_scaler.fit_transform(text_vectorized_df)\n",
    "\n",
    "# Initialize the PCA \n",
    "pca_method = PCA(n_components = 0.95, random_state = 10)\n",
    "\n",
    "\n",
    "# Apply PCA to the data and store the reduced dimensional data\n",
    "text_vectorized_dims_red = pca_method.fit_transform(text_vectorized_df_scaled)\n",
    "\n",
    "print(text_vectorized_dims_red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit KMeans after PCA\n",
    "kmeans_model_pca = KMeans\n",
    "print('For random choice of centroids initialization')\n",
    "inertia_plot(kmeans_model_pca, text_vectorized_dims_red, kmeans_init = 'random')\n",
    "\n",
    "### Fit KMeans++\n",
    "print('For k-means++ choice of centroids initialization')\n",
    "inertia_plot(kmeans_model_pca, text_vectorized_dims_red, kmeans_init = 'k-means++' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhoutte_scores_plots(kmeans_model_pca, text_vectorized_dims_red, start = 2, stop = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, fit the KMeans with the selected \"K\" clusters and visualize the data samples and clusters\n",
    "\n",
    "optimal_k_pca = 6\n",
    "\n",
    "\n",
    "_ = fit_and_plot(kmeans_model(n_clusters = optimal_k_pca), X = text_vectorized_dims_red, k_selected = optimal_k_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hierarchical Clustering\n",
    "\n",
    "Agglomerative clustering : [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "\n",
    "1. Unstructured:\n",
    "\n",
    "    - This is done using the \"affinity\" (similarity measure) and doesn't take connectivity constraint between the samples in to account. This results in undesired clusters in case of complex manifolds (Non-linear low dimensional curve)\n",
    "\n",
    "2. Structured: \n",
    "    \n",
    "    - with the KNeighbors constraint (computes the kneighbors for each sample)\n",
    "       \n",
    "       - this gives a good clustering taking closer samples into account rather than just based on the \"euclidean\" distance measure\n",
    "\n",
    "For more details: [unstructured vs structured agglomerative clustering](https://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py)\n",
    "\n",
    "For KNeighbors graph computation : [KNeighbors_graph](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph)\n",
    "\n",
    "Metrics: \n",
    "\n",
    "1. Silhoutte scores\n",
    "\n",
    "2. Dendrogram plot - Mostly, for Unstructured samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute clusters with different linkage and clusters \n",
    "def agg_model(X, cluster_limit = 10 , linkage = None):\n",
    "    # function returns optimal \"k\" value for number of clusters\n",
    "    optimal_k_agg_unstruct = None\n",
    "    silhoutte_score_ = -1\n",
    "    \n",
    "    ## Initialize and fit the agglomerative model\n",
    "    for cluster_size in range (2, cluster_limit):\n",
    "        agglomerative_model = AgglomerativeClustering(n_clusters= cluster_size, \n",
    "                                                  linkage= linkage).fit(X)\n",
    "\n",
    "        ## Compute and print the silhoutte score \n",
    "        silhoutte_score_var = silhouette_score(X, agglomerative_model.labels_)\n",
    "        print('Silhouette-Score for', cluster_size,  'Clusters: ', silhoutte_score_var)\n",
    "\n",
    "        if silhoutte_score_var > silhoutte_score_ :\n",
    "            silhoutte_score_ = silhoutte_score_var\n",
    "            optimal_k_agg_unstruct = cluster_size\n",
    "    return optimal_k_agg_unstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply the agglomerative clustering to data\n",
    "print(\"for the linkage type : ward\")\n",
    "optimal_k_agg_unstruct = agg_model(text_vectorized_dims_red, cluster_limit = 10, linkage = 'ward')\n",
    "\n",
    "### \n",
    "print(\"for the linkage type : complete\")\n",
    "optimal_k_agg_unstruct_complete = agg_model(text_vectorized_dims_red, cluster_limit = 10, linkage = 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the best model and visualize the clusters on scatter plot\n",
    "\n",
    "agglomerative_model_picked = AgglomerativeClustering(n_clusters= optimal_k_agg_unstruct_complete, \n",
    "                                              linkage='complete').fit(text_vectorized_dims_red)\n",
    "\n",
    "_ = fit_and_plot(agglomerative_model_picked, text_vectorized_dims_red, k_selected = optimal_k_agg_unstruct_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks okay. But there is no interested clusters in that we can infer from the plot and the clusters are really big. Silhoutte scores are also very low and almost insignificant for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Structured agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "### change this value for computing the kneighbors graph\n",
    "n_neighbors_considered = 4\n",
    "\n",
    "### Compute the connectivity graph (matrix form, might be a Compressed Sparse Row (CSR)) for each of the samples using KNeighbors method \n",
    "connectivity_graph = kneighbors_graph(text_vectorized_dims_red, \n",
    "                                      n_neighbors = n_neighbors_considered, \n",
    "                                      mode = 'connectivity', \n",
    "                                      metric = 'minkowski', p = 2, \n",
    "                                      include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute clusters with different linkage and clusters \n",
    "\n",
    "\n",
    "def agg_connectivity_model(X, cluster_limit = 10 , affinity = 'euclidean', connectivity_graph = None, linkage = None, use_connectivity = False):\n",
    "    # function returns optimal \"k\" value for number of clusters\n",
    "    optimal_k_agg_unstruct = None\n",
    "    silhoutte_score_ = -1\n",
    "    ## Initialize and fit the agglomerative model\n",
    "    for cluster_size in range (2, cluster_limit):\n",
    "        if (use_connectivity == False) :\n",
    "            agglomerative_model = AgglomerativeClustering(n_clusters= cluster_size, \n",
    "                                                  linkage= linkage).fit(X)\n",
    "        else:\n",
    "        ### Initialize clustering model. Options for linkage are : {“ward”, “complete”, “average”, “single”}\n",
    "            agglomerative_model = AgglomerativeClustering(n_clusters= cluster_size , \n",
    "                                              affinity = affinity, ##“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”##\n",
    "                                              connectivity= connectivity_graph, \n",
    "                                              linkage=linkage).fit(X)\n",
    "\n",
    "        ## Compute and print the silhoutte score \n",
    "        silhoutte_score_var = silhouette_score(X, agglomerative_model.labels_)\n",
    "        print('Silhouette-Score for', cluster_size,  'Clusters: ', silhoutte_score_var)\n",
    "\n",
    "        if silhoutte_score_var > silhoutte_score_ :\n",
    "            silhoutte_score_ = silhoutte_score_var\n",
    "            optimal_k_agg_unstruct = cluster_size\n",
    "    return optimal_k_agg_unstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compute structured hierarchical clustering...\")\n",
    "\n",
    "optimal_k_agg_struct =agg_connectivity_model(text_vectorized_dims_red, cluster_limit = 10 , affinity = 'euclidean', connectivity_graph = connectivity_graph, linkage = 'complete', use_connectivity = True )\n",
    "\n",
    "print(\"Good number of clusters from the agglomerative clustering are : {} \" .format(optimal_k_agg_struct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_and_plot function to fit the best model and visualize the plot of clusters\n",
    "agglomerative_connectivity_model_picked = AgglomerativeClustering(n_clusters= optimal_k_agg_struct , \n",
    "                                              affinity = 'euclidean', ##“euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”##\n",
    "                                              connectivity= connectivity_graph, \n",
    "                                              linkage='ward').fit(text_vectorized_dims_red)\n",
    "\n",
    "_ = fit_and_plot(agglomerative_connectivity_model_picked, text_vectorized_dims_red, k_selected = optimal_k_agg_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative clustering methods are poorly performing on this type of data showing very low silhoutte scores and large cluster sizes. However, with the connectivity graph, agglomerative method tend to be better at clustering showing the good separated clusters from the \"TSNE\" plot compared to one without the connectivity graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gaussian Mixture models (GMM)\n",
    "\n",
    "Some other techniques : \n",
    "\n",
    "1. Bayesian Gaussian Mixture Models\n",
    "\n",
    "**Metrics :** \n",
    "\n",
    "Here, Best model is selected based on BIC since it is better suited metric for evaluating the Gaussian Mixture models. However, silhoutte scores will be displayed as well for reference.\n",
    "\n",
    "1. Silhoutte score\n",
    "\n",
    "2. BIC, AIC - Probabilistic model selection criteria\n",
    "    \n",
    "    - When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n",
    "\n",
    "**Note:** \n",
    "\n",
    "1. BIC: \n",
    "\n",
    "    - This reflects the model complexity. Model with lower Bayesian Information criteria is preferred. \n",
    "\n",
    "    - This is better suited than the Akaike Information criteria \n",
    "\n",
    "References : \n",
    "\n",
    "1. [wiki](https://en.wikipedia.org/wiki/Bayesian_information_criterion)\n",
    "\n",
    "2. [AIC vs BIC](https://machinelearningmastery.com/probabilistic-model-selection-measures/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to do Clustering using Gaussian Mixture Model\n",
    "def gaussian_mixture_model(X, cluster_limit = 10 ):\n",
    "    # function returns optimal \"k\" value for number of clusters\n",
    "    optimal_k_gmm = None\n",
    "    best_gmm = None\n",
    "    best_cv_type = None\n",
    "    bic = []\n",
    "    lowest_bic = np.infty\n",
    "    silhoutte_score_ = -np.infty\n",
    "    cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "    ## Initialize and fit the model\n",
    "    for cluster_size in range (2, cluster_limit):\n",
    "        for cv_type in cv_types:\n",
    "            # Fit a Gaussian mixture with EM\n",
    "            gmm = GaussianMixture(n_components= cluster_size,\n",
    "                                          covariance_type=cv_type, random_state = 10)\n",
    "            gmm.fit(X)\n",
    "            bic.append(gmm.bic(X))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "                optimal_k_gmm = cluster_size\n",
    "                best_cv_type = cv_type\n",
    "                \n",
    "\n",
    "            ## Compute and print the silhoutte score \n",
    "            silhoutte_score_var = silhouette_score(X, best_gmm.fit_predict(X))\n",
    "            print('BIC for', cluster_size,  'Clusters and', cv_type, 'covariance type is : ', bic[-1])\n",
    "            print('Silhouette-Score for', cluster_size,  'Clusters and', cv_type, 'covraince type is : ', silhoutte_score_var)\n",
    "\n",
    "            if silhoutte_score_var > silhoutte_score_ :\n",
    "                silhoutte_score_ = silhoutte_score_var\n",
    "                \n",
    "                \n",
    "    return best_gmm, optimal_k_gmm , best_cv_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gmm_model_selected, optimal_k_gmm, best_cov_type = gaussian_mixture_model(text_vectorized_dims_red, cluster_limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gmm_model_selected.covariances_.shape\n",
    "print('BIC for best Gaussian Mixture model with ', optimal_k_gmm,  'Clusters and', best_cov_type, 'covariance type is : ', best_gmm_model_selected.bic(text_vectorized_dims_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the best model with good hyperparameters, fit the model to data and plot the clusters to visualize them\n",
    "\n",
    "\n",
    "_ = fit_and_plot(best_gmm_model_selected, text_vectorized_dims_red , k_selected = optimal_k_gmm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spectral Clustering \n",
    "\n",
    "**Data points in ML can be represented as the graphs**\n",
    "\n",
    "- we need datapoints and pairwise similarity scores(or distances or neighborhood or connections,...), construct the similairity graph.\n",
    "\n",
    "- However, in KNN graph, we take only k-nearest neighbours into acoount.\n",
    "\n",
    "Problem: \n",
    "\n",
    "    To find the optimal cut that partitions the graph into connected components.\n",
    "    \n",
    "Solution: \n",
    "\n",
    "    - The second smallest eigen value $$\\lambda_{2}$$ of the Laplacian matrix is the one that corresponds to the best optimal cut in the graph such that the nodes are assigned almost same on both sides.\n",
    "    \n",
    "    - Eigen vector corresponding to $$\\lambda_{2}$$ is the optimal choice and is called \"Fiedler Vector\". This vector takes the value either '+1' or '-1'.\n",
    "    \n",
    "Reyleigh Theorem: We relax the constraint of values in Fiedler vector, that is, we allow real numbers in Fiedler vector.\n",
    "    \n",
    "\n",
    "**Spectrum of graph :** It is the \"Eigen vectors\" of the graph ordered in \"increasing order of the eigen values\". This is referred as \"Spectral Embedding\" and projects the data into low dimensional space.\n",
    "\n",
    "**Spectral Embedding:** It is projection of the data points onto the eigen vectors space.\n",
    " \n",
    "**Affinity matrix:** \n",
    "\n",
    "    - Basically, it is a similarity matrix for mutual relation between the data points or nodes in a graph. Computed through the kernels or similarity measures.\n",
    "    \n",
    "    - Used to get connectivity between the nodes.\n",
    "    \n",
    "**Note:** If the affinity is adjancency matrix of the graph, then spectral clustering can be used to find normalized graph cuts.\n",
    "\n",
    "\n",
    "reference : [affinity matrix explanation](https://deepai.org/machine-learning-glossary-and-terms/affinity-matrix#:~:text=An%20Affinity%20Matrix%2C%20also%20called,a%20set%20of%20data%20points.)  \n",
    "\n",
    "\n",
    "**Graph Laplacian matrix:**\n",
    "    \n",
    "    - It is used mathematically to capture the useful properties of the graph \n",
    "      \n",
    "$$ L = D - A $$\n",
    "\n",
    "Properties: \n",
    "\n",
    "    - trivial eigen pair, that is, $$eigen value, \\lambda = 0 & eigen vector, \\x = (1......1)$$\n",
    "    \n",
    "    - symmetric matric and positive semi-definite \n",
    "    \n",
    "    - Eigen values are greater than or equal to zero\n",
    "\n",
    "**Degree Matrix (D):** \n",
    "\n",
    "    It is matrix in which diagonal elements of each node represents the degree for corresponding node of graph and all other elements are zero.\n",
    "    \n",
    "**Adjacency Matrix (A):**\n",
    "\n",
    "    This matrix represents the adjacency relation between nodes through the edge weights or edge presence (value of 1). \n",
    "\n",
    "Types :\n",
    "\n",
    "1. Unnormalized\n",
    "\n",
    "2. Normalized -  \n",
    "\n",
    "    i. Random walk (shi & Malik) - Uses generalized Eigen vectors\n",
    "    \n",
    "    ii. Symmetric (Andrew Ng) - Uses affinity matrix with Gaussian Kernel (a.k.a RBF), takes \"k\" largest eigenvectors instead of smallest and normalizes the rows of embedding vector to have unit length before clustering is performed.\n",
    "\n",
    "Reason to use Normalized clustering : \n",
    "\n",
    "    - It helps in obtaining, both the Within-cluster similarity (WSS) and Between-cluster similarity score (BSS) \n",
    "    \n",
    "    - Better to use the \"Elbow Method\" to choose the right \"K\" for number of clusters.\n",
    "\n",
    "Metrics : \n",
    "\n",
    "1. Silhoutte score\n",
    "\n",
    "2. Conductance -\n",
    "    \n",
    "    - lower the conductance, better is the clustering and resembles the balanced partitions.\n",
    "    \n",
    "    - Low conductance cut means more internal edges in the clusters and less edges between clusters.\n",
    "\n",
    "3. Eigengap heuristic - \n",
    "\n",
    "    - Eigengap heuristic suggests the number of clusters k is usually given by the value of 'k' that maximizes the eigengap (difference between consecutive eigenvalues).\n",
    "    \n",
    "    - The larger this eigengap is, the closer the eigenvectors of the ideal case and hence the better spectral clustering works.\n",
    "\n",
    "    - This is not resilient to noise in the data\n",
    "\n",
    "Since, it is far tedious to compute the conductance of graph cut, here, I will be evaluating the model performance through \"Eigengap\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of Spectral clustering :\n",
    "\n",
    "1. Preprocess - construct the matrix representation of graph (Laplacian , L)\n",
    "\n",
    "2. Decomposition - (EigenDecomposition)\n",
    "\n",
    "    i. Find Eigen values and Eigen vectors of the Laplacian\n",
    "\n",
    "    ii. Map vertices to the low-dimensional representation based on one or more eigen vectors\n",
    "\n",
    "3. Grouping -\n",
    "\n",
    "    Assign the points to two or more clusters, based on the new representation\n",
    "    \n",
    "Note: Steps 1 and 2 result in the spectral embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-way spectral clustering: \n",
    "\n",
    "What if we want to find the \"k\" clusters instead of 2 clusters?\n",
    "\n",
    "Two basic approaches :\n",
    "\n",
    "1. Recursive bi-partitioning - \n",
    "\n",
    "        - This means partitioning clusters in clusters recursively\n",
    "        \n",
    "        - This is not the preferred since the cut will be unstable and looks like hierarchical divisive manner\n",
    "\n",
    "2. Well, we can solve this using the more eigen vectors for representation and run \"KMeans\" clustering algorithm. (Shi-Malik algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from networkx.algorithms.cuts import conductance\n",
    "from scipy.sparse import csgraph\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to compute the affinity matrix\n",
    "\n",
    "def affinity_matrix_func(input_matrix, distance_metric = 'euclidean', affinity_metric = 'rbf', gamma = 1.0):\n",
    "    \n",
    "    ## Compute the distance matrix from the given input matrix\n",
    "    distance_matrix = squareform(pdist(input_matrix, metric = 'euclidean'))\n",
    "    \n",
    "    ### Compute the affinity matrix from distance matrix using the 'RBF' kernel\n",
    "    if affinity_metric == 'rbf':\n",
    "        affinity_matrix = np.exp(-gamma * distance_matrix ** 2)\n",
    "    else : \n",
    "        print(\"choose the right option for \\'affinity_metric\\' \")\n",
    "    \n",
    "    return affinity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to compute the eigendecomposition and eigengap.\n",
    "\n",
    "def eigengap_k(A, plot = True, num_top_k_choices = 1, num_eigenvalues_considered = 100):\n",
    "    \"\"\"\n",
    "    :param A: Affinity matrix (also called similarity matrix)\n",
    "    :param plot: plots the sorted eigen values for visual inspection\n",
    "    :param num_eigenvalues_considered: \n",
    "            This we need becuase sometime eigen values drop drastically as number of eigen values approach the rank of matrix.\n",
    "            Also, we don't need number of clusters to be nearly as the number of data points which is not interesting and useful for analysis.\n",
    "    :return A tuple containing:\n",
    "    - the optimal number of clusters by eigengap heuristic\n",
    "    - all eigen values\n",
    "    - all eigen vectors\n",
    "    \n",
    "    This method performs the eigen decomposition on a given affinity matrix,\n",
    "    following the steps recommended in the paper:\n",
    "    1. Construct the normalized affinity matrix: L = D−1/2ADˆ −1/2.\n",
    "    2. Find the eigenvalues and their associated eigen vectors\n",
    "    3. Identify the maximum gap which corresponds to the number of clusters\n",
    "    by eigengap heuristic\n",
    "\n",
    "    \"\"\"\n",
    "    L = csgraph.laplacian(A, normed=True)\n",
    "    \n",
    "    n_components = A.shape[0]\n",
    "    \n",
    "    eigenvalues, eigenvectors = LA.eig(L)\n",
    "    \n",
    "    print('Number of eigen values are : ' , len(eigenvalues))\n",
    "    \n",
    "    ### Sort the eigen values and vectors in descending order.\n",
    "    sort_indicies = np.flip(eigenvalues.argsort()) # In descending order and argsort here gives ascending order\n",
    "    eigenvalues_sorted = eigenvalues[sort_indicies]\n",
    "    eigenvectors_sorted = eigenvectors[:, sort_indicies]\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        plt.title('Largest eigen values of input matrix')\n",
    "        plt.scatter(np.arange(len(eigenvalues_sorted)), eigenvalues_sorted)\n",
    "        plt.xlabel('number of clusters \\\"k\\\" ')\n",
    "        plt.ylabel('eigen values')\n",
    "        plt.grid()\n",
    "        \n",
    "    # Identify the optimal number of clusters as the index corresponding\n",
    "    # to the larger gap between eigen values\n",
    "    \n",
    "    ### This below code eliminates the eigen values that are at position equal to the number of samples \n",
    "    ### because we don't need each cluster is assigned to only few points and\n",
    "    ### that particular clustering is often not useful and interesting\n",
    "    if num_eigenvalues_considered >= A.shape[0]:\n",
    "        num_eigenvalues_considered = math.floor((A.shape[0])/2)\n",
    "    \n",
    "    eigen_gaps_indicies_sorted = np.argsort(np.abs(np.diff(eigenvalues_sorted[: num_eigenvalues_considered])))[::-1][:num_top_k_choices]\n",
    "\n",
    "    opt_num_clusters = eigen_gaps_indicies_sorted + 1 ### Add \"1\" since the argsort reduces number of indicies by 1\n",
    "    \n",
    "    ### Remove \"1\" from cluster numbers since we don't need only one cluster of big size and it's nothing useful for analysis.\n",
    "    opt_num_clusters = opt_num_clusters[ opt_num_clusters >= 2 ]\n",
    "    \n",
    "    return opt_num_clusters, eigenvalues_sorted, eigenvectors_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to do spectral clustering. Here, I have implemented the Andrew Ng's algorithm. \n",
    "\n",
    "def spectral_clustering_model(X, cluster_limit = 11 , gamma = 1.0, affinity = 'rbf' , assign_labels= 'kmeans'):\n",
    "    # function returns optimal \"k\" value for number of clusters\n",
    "    optimal_k_spec = None\n",
    "    silhoutte_score_ = -1\n",
    "    best_spectral_model = None\n",
    "    \n",
    "    ### First, compute affinity matrix \n",
    "    affinity_matrix = affinity_matrix_func(X, distance_metric = 'euclidean', affinity_metric = 'rbf', gamma = 1.0 )\n",
    "    \n",
    "    \n",
    "    ### Second, use the affinity matrix to determine the right choice for \"k\"\n",
    "    cluster_sizes, _, _ = eigengap_k(affinity_matrix, plot = True, num_top_k_choices = 5, num_eigenvalues_considered = 100 )\n",
    "    print('Right choices of \\'k\\' for spectral clustering through Eigengap heuristic would be (in decreasing order of importance): ', cluster_sizes)\n",
    "    \n",
    "    cluster_size = cluster_sizes[0]\n",
    "    \n",
    "    ### Third, fit the spectral clustering model with selected \"k\" for number of clusters\n",
    "    \n",
    "    #for cluster_size in cluster_sizes:\n",
    "        ### Initialize clustering model.\n",
    "    spectral_cluster_model = SpectralClustering(n_clusters= cluster_size, \n",
    "                                                eigen_solver='arpack',# Number of eigen vectors in embedding space\n",
    "                                                random_state=10, \n",
    "                                                gamma = gamma,                 # Kernel coefficient - defines the strength or spread of the kernel\n",
    "                                                affinity='rbf',  \n",
    "                                                assign_labels = assign_labels,\n",
    "                                               n_jobs = -1)\n",
    "\n",
    "    spectral_cluster_model.fit(X)\n",
    "\n",
    "    ## Compute and print the silhoutte score \n",
    "    silhoutte_score_var = silhouette_score(X, spectral_cluster_model.labels_)\n",
    "    print('Silhouette-Score for', cluster_size,  'Clusters: ', silhoutte_score_var)\n",
    "\n",
    "    if silhoutte_score_var > silhoutte_score_ :\n",
    "                silhoutte_score_ = silhoutte_score_var\n",
    "                optimal_k_spec = cluster_size\n",
    "                best_spectral_model = spectral_cluster_model                \n",
    "                \n",
    "    return best_spectral_model, optimal_k_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note: use the appropriate data since we have to find the spectral embedding for the data samples during clustering.\n",
    "\n",
    "best_spectral_model_selected, optimal_k_spectral = spectral_clustering_model(text_vectorized_dims_red, cluster_limit = 10, gamma = 1.0 , affinity = 'rbf', assign_labels = 'kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the best model with good hyperparameters, fit the model to data and plot the clusters to visualize them\n",
    "\n",
    "tsne_embedded_data = fit_and_plot(best_spectral_model_selected, text_vectorized_dims_red , k_selected = optimal_k_spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the predicted cluster labels to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df['y'] = best_spectral_model_selected.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling on each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_selected = optimal_k_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorization(df, num_clusters):\n",
    "    \n",
    "    vectorizers = []\n",
    "\n",
    "    for x in range(0, num_clusters):\n",
    "        # Creating a vectorizer\n",
    "        vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))\n",
    "    \n",
    "    vectorized_data = []\n",
    "\n",
    "    for current_cluster, cvec in enumerate(vectorizers):\n",
    "        try:\n",
    "            vectorized_data.append(cvec.fit_transform(df.loc[df['y'] == current_cluster, 'processed_text']))\n",
    "        except Exception as e:\n",
    "            print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
    "            vectorized_data.append(None)\n",
    "            \n",
    "    return vectorized_data, vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters_vectorized_data, vectorizers = count_vectorization(filtered_data_df, num_clusters_selected )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_models(vectorized_data, num_clusters, num_topics_per_cluster = 20):\n",
    "    # number of topics per cluster\n",
    "    NUM_TOPICS_PER_CLUSTER = num_topics_per_cluster\n",
    "\n",
    "    lda_models = []\n",
    "    for x in range(0, num_clusters):\n",
    "        # Latent Dirichlet Allocation Model\n",
    "        lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n",
    "        lda_models.append(lda)\n",
    "\n",
    "    # lda_models[0]\n",
    "    \n",
    "    clusters_lda_data = []\n",
    "\n",
    "    for current_cluster, lda in enumerate(lda_models):\n",
    "        # print(\"Current Cluster: \" + str(current_cluster))\n",
    "        if vectorized_data[current_cluster] != None:\n",
    "            clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))\n",
    "    \n",
    "    return clusters_lda_data, lda_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=3):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "                \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lda_data, lda_models = lda_models(clusters_vectorized_data, num_clusters_selected, num_topics_per_cluster = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df['keywords'] = np.nan\n",
    "filtered_data_df['keywords'] = filtered_data_df['keywords'].astype(np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for cluster_id, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "\n",
    "    if clusters_vectorized_data[cluster_id] != None:\n",
    "        print(\"Perplexity score of lda model for \", cluster_id , \"cluster is : \" , lda.perplexity(clusters_vectorized_data[cluster_id]))\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[cluster_id]))\n",
    "        filtered_data_df['keywords'] = np.where((filtered_data_df['y'] == cluster_id), str(all_keywords[-1]), filtered_data_df['keywords'] )\n",
    "    else : \n",
    "        filtered_data_df['keywords'] = np.where((filtered_data_df['y'] == cluster_id), None, filtered_data_df['keywords'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df.loc[filtered_data_df['y'] == 18, 'keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the spectral_model\n",
    "pickle.dump(best_spectral_model_selected, open(\"output/spectral_model.pkl\", \"wb\" ))\n",
    "\n",
    "# save the topic model (LDA model)\n",
    "pickle.dump(lda_models, open(\"output/lda_models.pkl\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = filtered_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"tsne_embedding_coord1\"] = tsne_embedded_data[:, 0]\n",
    "results_df[\"tsne_embedding_coord2\"] = tsne_embedded_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('output/results_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "We choose this type of problem because human behaviour is comlex and varies hugely from one to other. Thus, dealing this kind of complicated problem with the tools that we have in our hand gives a better understanding of the algorithms.\n",
    "\n",
    "In conclusion, we practiced our data preprocessing and analysis that data scientists and ML engineers do. We looked at different metrics for evaluating how good the clustering was like silhouette scores and inertia, BIC. Then we found that the best clustering models were generally once we reduced the data down to lower dimensions and 4-5 clusters in the data.\n",
    "\n",
    "Due to the limitations of the data, clustering was less than optimal, but in the future we will know what to look for when we consider clustering a new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
